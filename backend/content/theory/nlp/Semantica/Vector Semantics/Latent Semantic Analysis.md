# üìö Analisi Semantica Latente (Latent Semantic Analysis - LSA)

## üß† Concetti chiave

LSA √® una tecnica di *apprendimento non supervisionato* utilizzata nel *Natural Language Processing (NLP)* per scoprire **argomenti latenti** nascosti all‚Äôinterno di grandi collezioni di testi. √à connessa ai seguenti concetti matematici fondamentali:

- üìâ **Riduzione dimensionale**: passaggio da uno spazio vettoriale ad alta dimensione ($\mathbb{R}^N$) a uno spazio ridotto ($\mathbb{R}^M$, con $M \ll N$).
- üî¢ **[[Singular Value Decomposition|Scomposizione ai valori singolari (SVD)]]**: tecnica algebrica per decomporre matrici.
- üîç **Scoperta di argomenti latenti**: estrazione non supervisionata di strutture semantiche nei dati testuali.
- üì¶ **Densificazione delle rappresentazioni**: da rappresentazioni sparse e lunghe a vettori corti e densi.
## üßæ Matrice termine-documento

Sia $\mathbf{X} \in \mathbb{R}^{|D| \times |V|}$ una matrice termine-documento o una matrice TF-IDF.  
Dove:

- $|D|$: numero di documenti
- $|V|$: dimensione del vocabolario (numero di termini distinti)

Ogni elemento $x_{ij}$ di $\mathbf{X}$ rappresenta l‚Äôimportanza del termine $j$ nel documento $i$.

### üìå La matrice $\mathbf{X}$ cattura:
- Relazioni **termine vs documento**: quanto √® rappresentativo un termine in un documento.
- Relazioni **termine vs termine**: se due termini co-occorrono nei documenti.
- Relazioni **documento vs documento**: similarit√† semantica tra documenti.
## üß† Idea chiave n.1

> **Esistono $k$ argomenti latenti nascosti nella matrice $\mathbf{X}$, che vogliamo scoprire.**

Questi argomenti non sono osservabili direttamente, ma possono emergere come combinazioni lineari di parole e documenti attraverso l'SVD.
## üîç Idea chiave n.2

> **Non considerare $\mathbf{X}$ solo come dati grezzi: decompone la matrice in componenti strutturate.**

La decomposizione serve per **estrarre informazione strutturale** e ridurre la dimensionalit√† mantenendo le componenti principali.
## üßÆ Decomposizione con SVD

La decomposizione ai valori singolari √®:

$$
\mathbf{X} = \mathbf{U} \mathbf{S} \mathbf{V}^\top
$$

dove:

- $\mathbf{U} \in \mathbb{R}^{|D| \times |D|}$: **matrice dei documenti**, ortonormale
- $\mathbf{S} \in \mathbb{R}^{|D| \times |V|}$: **matrice diagonale** dei valori singolari (importanza degli assi)
- $\mathbf{V} \in \mathbb{R}^{|V| \times |V|}$: **matrice dei termini**, ortonormale

## ‚úÇÔ∏è Approssimazione a rango ridotto (Truncated SVD)

Spesso $\mathbf{X}$ √® molto grande. Usiamo una versione **troncata**:

$$
\mathbf{X}_k = \mathbf{U}_k \mathbf{S}_k \mathbf{V}_k^\top
$$

dove:

- $k \ll \min(|D|, |V|)$
- $\mathbf{U}_k \in \mathbb{R}^{|D| \times k}$
- $\mathbf{S}_k \in \mathbb{R}^{k \times k}$
- $\mathbf{V}_k \in \mathbb{R}^{|V| \times k}$

In questo modo otteniamo:

- **Vettori documenti** proiettati in $\mathbb{R}^k$, tramite $\mathbf{U}_k \mathbf{S}_k$
- **Vettori termini** proiettati in $\mathbb{R}^k$, tramite $\mathbf{V}_k \mathbf{S}_k$


### üîí Vincoli:

$$
\begin{aligned}
&\min_{\mathbf{U_k}, \mathbf{S_k}, \mathbf{V_k}} \|\mathbf{X} - \mathbf{U_k S_k V_k}^\top\|_F \\
&\text{s.t. } \mathbf{U_k}^\top \mathbf{U_k} = \mathbf{I_k}, \quad \mathbf{V_k}^\top \mathbf{V_k} = \mathbf{I_k} \\
&\mathbf{S_k} = \operatorname{diag}(\sigma_1, \sigma_2, \dots, \sigma_r) \quad \text{con } \sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_k \geq 0
\end{aligned}
$$

- $\|\cdot\|_F$: norma di Frobenius (somma dei quadrati di tutti gli elementi della matrice)
- I vincoli garantiscono che $\mathbf{U}_k$ e $\mathbf{V}_k$ siano **matrici ortonormali**
- I $\sigma_i$ (valori singolari) sono **sempre reali e non negativi**.

<img src="../../../../../images/mysvd.png" width="75%" style="display: block; margin-left: auto; margin-right: auto;">

## üìå Interpretazione semantica

- Ogni **colonna di $\mathbf{V}_k$** rappresenta un *argomento latente*, ovvero una combinazione di termini che tende a comparire insieme nei documenti.
- Ogni **riga di $\mathbf{U}_k$** descrive un documento secondo la sua affinit√† con questi argomenti latenti.
- La matrice $\mathbf{S}_k$ scala ciascun asse latente in base alla sua **importanza** (varianza spiegata).

## üéØ Perch√© funziona?

- I **primi $k$ valori singolari** $\sigma_1, \dots, \sigma_k$ catturano **la maggior parte dell‚Äôinformazione semantica** presente in $\mathbf{X}$.
- I valori successivi rappresentano spesso **rumore** (parole rare, anomalie, incoerenze).
- La proiezione in $\mathbb{R}^k$ consente di cogliere **relazioni latenti** tra termini e documenti, anche se non co-occorrono esplicitamente.
- Questo processo prende il nome di **riduzione dimensionale semantica**: migliora l‚Äôanalisi mantenendo solo le componenti significative.

## üîΩ Riduzione della dimensionalit√†: Proiezione di un documento

Dato un documento $\mathbf{d} \in \mathbb{R}^{|V|}$ nello spazio originale dei termini:

$$ 
\mathbf{d}_k = \underbrace{\mathbf{V}_k^\top}_{k \times |V|} \underbrace{\mathbf{d}}_{|V| \times 1} \in \mathbb{R}^k
$$

- Stiamo **proiettando $\mathbf{d}$ nello spazio latente** generato da SVD.
- Questo spazio ha dimensione $k$, dove $k$ √® scelto per catturare solo le direzioni semantiche pi√π rilevanti.

## üß† Perch√© usare la SVD?

Sia $\mathbf{X}$ una matrice termini-documenti. Allora:

- $\mathbf{X}\mathbf{X}^\top$ misura le **correlazioni tra termini** (similitudine semantica).
- Applicando la SVD otteniamo:

$$
\mathbf{X} \mathbf{X}^\top = (\mathbf{U} \mathbf{S} \mathbf{V}^\top)(\mathbf{V} \mathbf{S}^\top \mathbf{U}^\top) = \mathbf{U} \mathbf{S} \mathbf{S}^\top \mathbf{U}^\top
$$

- Quindi stiamo **decomponendo la correlazione** tra termini (o tra documenti) in modo geometrico.

## üß≠ Interpretazione geometrica

La SVD pu√≤ essere vista come una **pipeline geometrica**:

$$
\mathbf{A}\mathbf{x} = \mathbf{U}(\mathbf{S}(\mathbf{V}^\top \mathbf{x}))
$$

| Fase         | Operazione               |
|--------------|--------------------------|
| 1Ô∏è‚É£ Rotazione | $\mathbf{V}^\top$        |
| 2Ô∏è‚É£ Scalatura | $\mathbf{S}$            |
| 3Ô∏è‚É£ Rotazione | $\mathbf{U}$            |

- Le **componenti principali** (valori singolari) indicano **quanto contribuisce ogni direzione**.
- La **scalatura + rotazioni** preserva struttura ma elimina ridondanza.

## ‚ö†Ô∏è Limiti di SVD

- La matrice $\mathbf{X}$ √® **molto sparsa** e ad alta dimensionalit√† ($10^5 \times 10^5$ o pi√π).
- L'SVD ha un **costo computazionale elevato** ($O(n^2)$).
- L‚Äôaggiunta di nuovi termini/documenti **richiede ricalcolo**.
- Serve **pre-elaborazione** (es. TF-IDF) per bilanciare la frequenza delle parole.

## üîö Conclusione

LSA consente di:
- Ridurre dimensionalit√† mantenendo la struttura semantica
- Rappresentare testi in uno spazio concettuale pi√π interpretabile
- Confrontare documenti e parole semanticamente
